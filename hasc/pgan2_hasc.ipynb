{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pgan2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python380jvsc74a57bd0d98adb0ea4a090799657e464d985c8879f4575dd89b168d44e006b042b3b2d36","display_name":"Python 3.8.0 64-bit"},"language_info":{"name":"python","version":"3.8.0"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"N1qKLHGvNg-U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620477565697,"user_tz":-60,"elapsed":1015,"user":{"displayName":"joury tasnim","photoUrl":"","userId":"11792332200639428744"}},"outputId":"51e78fc1-9502-4e55-b5ae-512c9692dc29"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iH0wj-wW6yh5"},"source":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jgdul_IqOFwA","outputId":"4871ccca-0368-4e01-b16f-80dc23406017"},"source":["#https://github.com/tzamalisp/Human-Activity-Recognition-with-Tensorflow2-and-Keras\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, LSTM, Dropout, Input, Conv1D, MaxPooling1D\n","import tensorflow as tf\n","import sys\n","import pandas as pd\n","import h5py\n","\n","np.set_printoptions(threshold=sys.maxsize)\n","\n","path = r'F:\\\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\data\\\\RL_acc_hasc_data_a_p.h5'\n","dataset = 'hasc'\n","\n","# - - - get data parameters and split - - - #\n","\n","hf = h5py.File(path, 'r')\n","X_source = np.array(hf.get('X'))\n","y_source = np.array(hf.get('y'))\n","p_source = np.array(hf.get('p'))\n","y_onehot_source = np.array(hf.get('y_onehot'))\n","p_onehot_source = np.array(hf.get('p_onehot'))\n","print(\"X source shape: {}\".format(X_source.shape))\n","print(\"Y source shape: {}\".format(y_source.shape))\n","print(\"P source shape: {}\".format(p_source.shape))\n","\n","# VARIABLES REGARDING DATA SHAPE, TRAINING\n","\n","seq_length = X_source.shape[1]\n","num_channels = X_source.shape[2]\n","input_shape = (seq_length, num_channels)\n","num_classes_a = y_onehot_source.shape[1]\n","num_classes_p = p_onehot_source.shape[1]\n","\n","# SPLIT INTO TRAINING AND VALIDATION SETS\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_source, y_source, test_size=0.7, random_state=42)\n","p_train, p_test, p_train_onehot, p_test_onehot = train_test_split(p_source, p_onehot_source, test_size=0.7, random_state=42)\n","y_train_onehot, y_test_onehot = train_test_split(y_onehot_source, test_size=0.7, random_state=42)\n","\n","# SPLIT INTO TRAINING AND VALIDATION SETS\n","#\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.30, random_state=42)\n","p_train, p_test, p_train_onehot, p_test_onehot = train_test_split(p_train, p_train_onehot, test_size=0.30, random_state=42)\n","y_train_onehot, y_test_onehot = train_test_split(y_train_onehot, test_size=0.30, random_state=42)\n","\n","latent_dim = num_channels #length of random input fed to generator\n","batch_size_train = y_train.shape[0] #num instances generated for G/D training\n","batch_size_test = y_test.shape[0]\n","epoch = 4683\n","vis_freq =100 #100\n","print(batch_size_train)\n","print(batch_size_test)\n","print(y_test_onehot.shape, y_train_onehot.shape, p_test_onehot.shape, p_train_onehot.shape)\n","#WEIGHTS FOR DIFFERENT TERMS IN THE LOSS FUNCTION\n","\n","D_p_loss_weight = 1\n","D_a_loss_weight = 1\n","\n","a_accuracy = 0.24\n","p_accuracy = 0.2\n","\n","# - - - build the model - - - #\n","\n","# - - - build the model - - - #\n","\n","def generator(seq_length, latent_dim):\n","    G_input_shape = (seq_length, latent_dim)\n","    G_in = Input(shape=G_input_shape)\n","    G = Dropout(.5)(G_in)\n","    G = LSTM(128, return_sequences=True, activation=\"tanh\")(G)#uci 128\n","    G = Dropout(.5)(G)\n","    G = Dense(latent_dim, activation=\"tanh\")(G)\n","    G = Model(inputs=G_in, outputs=G, name='generator')\n","    G.summary()\n","    return G\n","\n","def descriminatorA(input_shape, num_classes):\n","    \n","    disc = Sequential(name=\"D_a\")\n","    \n","    # Conv1D layer - Input layer --> 01\n","    disc.add(keras.layers.Conv1D(filters= 64, kernel_size= 3, activation= 'relu', \n","                                  input_shape=input_shape, padding= 'valid' , strides= 1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    disc.add(keras.layers.LeakyReLU(alpha=0.1))\n","    \n","    # MaxPooling1D layer --> 01\n","    disc.add(keras.layers.MaxPooling1D(pool_size=2,padding = 'valid'))\n","    \n","    # Dropout layer --> 01\n","    disc.add(keras.layers.Dropout( 0.5))\n","    \n","    # Batch Normalization Layer - Applied just efore the activation functions \n","    disc.add(keras.layers.BatchNormalization())\n","    \n","    # Conv1D layer  --> 02\n","    disc.add(keras.layers.Conv1D(filters=64, kernel_size=3,activation='relu',padding='valid',strides=1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    disc.add(keras.layers.LeakyReLU(alpha=0.1))\n","    \n","    # MaxPooling1D layer --> 01\n","    disc.add(keras.layers.MaxPooling1D(pool_size= 2,padding= 'valid'))\n","\n","    # Dropout layer --> 01\n","    disc.add(keras.layers.Dropout(0.5))\n","\n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    disc.add(keras.layers.BatchNormalization())\n","\n","    # Conv1D layer  --> 03\n","    disc.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', strides=1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    disc.add(keras.layers.LeakyReLU(alpha=0.1))\n","\n","    # MaxPooling1D layer --> 01\n","    disc.add(keras.layers.MaxPooling1D(pool_size=2,padding='valid'))\n","    \n","    # Dropout layer --> 01\n","    disc.add(keras.layers.Dropout(0.5))\n","\n","    # Conv1D layer  --> 03\n","    disc.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', strides=1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    disc.add(keras.layers.LeakyReLU(alpha=0.1))\n","\n","    # MaxPooling1D layer --> 01\n","    disc.add(keras.layers.MaxPooling1D(pool_size=2,padding='valid'))\n","    \n","    # Dropout layer --> 01\n","    disc.add(keras.layers.Dropout(0.5))\n","\n","    # Flatten layer --> 01\n","    disc.add(keras.layers.Flatten())\n","\n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    disc.add(keras.layers.BatchNormalization())\n","\n","    # Dense layer --> 01\n","    disc.add(keras.layers.Dense(units=100, activation='relu' ))\n","\n","    # Leaky ReLu Layer --> 01\n","    disc.add(keras.layers.LeakyReLU(alpha=0.1))\n","\n","    # Dropout layer --> 01\n","    disc.add(keras.layers.Dropout(0.5)) \n","    \n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    disc.add(keras.layers.BatchNormalization())\n","\n","    # Dense layer - Output layer  --> 02\n","    disc.add(keras.layers.Dense(num_classes, activation='softmax') )\n","    \n","    disc.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    disc.summary()\n","    return disc\n","\n","def descriminatorP(input_shape, num_classes):\n","    classifier = Sequential(name=\"D_p\")\n","    # Conv1D layer - Input layer --> 01\n","    classifier.add(keras.layers.Conv1D(filters= 64, kernel_size=3, activation= 'relu', \n","                                  input_shape=input_shape, padding= 'valid' , strides= 1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    classifier.add(keras.layers.LeakyReLU(alpha=0.1))\n","    \n","    # MaxPooling1D layer --> 01\n","    classifier.add(keras.layers.MaxPooling1D(pool_size=2,padding = 'valid'))\n","    \n","    # Dropout layer --> 01\n","    classifier.add(keras.layers.Dropout(0.5))\n","    \n","    # Batch Normalization Layer - Applied just efore the activation functions \n","    classifier.add(keras.layers.BatchNormalization())\n","    \n","    # Conv1D layer  --> 02\n","    classifier.add(keras.layers.Conv1D(filters=64, kernel_size=3,activation='relu',padding='valid',strides=1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    classifier.add(keras.layers.LeakyReLU(alpha=0.1))\n","    \n","    # MaxPooling1D layer --> 01\n","    classifier.add(keras.layers.MaxPooling1D(pool_size= 2,padding= 'valid'))\n","\n","    # Dropout layer --> 01\n","    classifier.add(keras.layers.Dropout(0.5))\n","\n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    classifier.add(keras.layers.BatchNormalization())\n","\n","    # Conv1D layer  --> 03\n","    classifier.add(keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='valid', strides=1))\n","    \n","    # Leaky ReLu Layer --> 01\n","    classifier.add(keras.layers.LeakyReLU(alpha=0.1))\n","\n","    # MaxPooling1D layer --> 01\n","    classifier.add(keras.layers.MaxPooling1D(pool_size=2,padding='valid'))\n","    \n","    # Dropout layer --> 01\n","    classifier.add(keras.layers.Dropout(0.5))\n","\n","    # Flatten layer --> 01\n","    classifier.add(keras.layers.Flatten())\n","\n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    classifier.add(keras.layers.BatchNormalization())\n","\n","    # Dense layer --> 01\n","    classifier.add(keras.layers.Dense(units=100, activation='relu' ))\n","\n","    # Leaky ReLu Layer --> 01\n","    classifier.add(keras.layers.LeakyReLU(alpha=0.1))\n","\n","    # Dropout layer --> 01\n","    classifier.add(keras.layers.Dropout(0.5)) \n","\n","    # Batch Normalization Layer - Applied just efore the activation functions\n","    classifier.add(keras.layers.BatchNormalization())\n","\n","    # Dense layer - Output layer  --> 02\n","    classifier.add(keras.layers.Dense(num_classes, activation='softmax') )\n","    \n","    classifier.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","    classifier.summary()\n","    return classifier\n","\n","\n","def create_gan(D_a, D_p, G):\n","    for layer in D_a.layers:\n","        layer.trainable = False\n","    for layer in D_p.layers:\n","        layer.trainable = False\n","    GDD = Model(inputs=G.input, outputs=[D_a(G.output), D_p(G.output)])\n","    GDD.compile(loss={\"D_a\":\"categorical_crossentropy\",\"D_p\":\"categorical_crossentropy\"},\n","                optimizer=\"adam\", metrics={\"D_a\":\"accuracy\",'D_p':\"accuracy\"},\n","                loss_weights = { \"D_a\": D_a_loss_weight,\"D_p\": D_p_loss_weight})\n","    GDD.summary()\n","    return GDD\n","\n","def create_generated_data(G):\n","    G_D= Model(inputs= G.input, outputs= G.output)\n","    G_D.summary()\n","    return G.output\n","\n","\n","#FUNCTION FOR TRAINING GENERATOR FROM DBOTH DISCRIMINATOR AND CLASSIFIER OUTPUT\n","\n","def train_G(batch_size, x, y_onehot, p_onehot, model, latent_dim):\n","\n","    loss = model.train_on_batch(x, [y_onehot,np.zeros_like(p_onehot)])#`train_on_batch` trains the model and updates the weights.loss = model.train_on_batch(x, [y_onehot, np.zeros_like(p_onehot)])\n","    return loss\n","\n","#CREATE GENERATOR AND DISCRIMINATOR\n","\n","generator_model= generator(seq_length, latent_dim)\n","discriminator_a_model = descriminatorA(input_shape, num_classes_a)\n","discriminator_p_model = descriminatorP(input_shape, num_classes_p)\n","generated_data_model = create_generated_data(generator_model)\n","\n","#CREATE FULL ARCHITECTURE WHERE OUPUT OF GENERATOR IS FED TO DISCRIMINATOR AND CLASSIFIER\n","\n","\n","\n","gan = create_gan(discriminator_a_model, discriminator_p_model, generator_model)\n","test_loss_list = []\n","train_loss_list = []\n","\n","gan.load_weights(\"F:\\PHD\\GAN\\code.12.2020\\PGAN1\\hasc\\\\results\\gan_ckpt_hasc/variables/variables\")\n","generator_model.load_weights(\"F:\\PHD\\GAN\\code.12.2020\\PGAN1\\hasc\\\\results\\generator_ckpt_hasc/variables/variables\")\n","discriminator_p_model.load_weights(\"F:\\PHD\\GAN\\code.12.2020\\PGAN1\\hasc\\\\results\\discriminator_p_ckpt_hasc/variables/variables\")\n","discriminator_a_model.load_weights(\"F:\\PHD\\GAN\\code.12.2020\\PGAN1\\hasc\\\\results\\discriminaror_a_ckpt_hasc/variables/variables\")\n","\n","\n","while epoch <= 1000000:\n","    \n","    train_loss = train_G(batch_size_train, X_train, y_train_onehot, p_train_onehot, gan, latent_dim)\n","    train_loss_list.append((epoch,train_loss[0],train_loss[1],train_loss[2],train_loss[3],train_loss[4] ))\n","    print(epoch, 'train', train_loss[3])# gan.metrics_names,\n","    \n","    test_loss = gan.test_on_batch(X_test, [y_test_onehot, p_test_onehot])\n","    test_loss_list.append((epoch,test_loss[0],test_loss[1],test_loss[2],test_loss[3],test_loss[4] ))\n","    print(epoch, 'test',  test_loss[3])#gan.metrics_names,\n","    if test_loss[3] > a_accuracy and test_loss[4] <=p_accuracy:\n","            gen_out = generator_model.predict(X_test)\n","            \n","            # save generated data\n","            \n","            h5f = h5py.File('F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\GN_'+ dataset+'_data'+str(epoch)+'_a_'+str(\"{:.2f}\".format(test_loss[3]) )+'_p_'+str(\"{:.2f}\".format(test_loss[4]))+'.h5', 'w')\n","            h5f.create_dataset('X', data=gen_out)\n","            h5f.create_dataset('y', data=y_test)\n","            h5f.create_dataset('y_onehot', data=y_test_onehot)\n","            h5f.create_dataset('p', data=p_test)\n","            h5f.create_dataset('p_onehot', data=p_test_onehot)\n","            h5f.close()\n","\n","            a_accuracy = test_loss[3]\n","            gan.save(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\gan_ckpt_\"+dataset)\n","            discriminator_a_model.save(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\discriminaror_a_ckpt_\"+dataset)\n","            discriminator_p_model.save(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\discriminator_p_ckpt_\"+dataset)\n","            generator_model.save(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\generator_ckpt_\"+dataset)\n","    \n","    if epoch % vis_freq == 0:\n","        pd.DataFrame(train_loss_list).to_csv(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\train_loss_v2\"+dataset+\".csv\")\n","        pd.DataFrame(test_loss_list).to_csv(\"F:\\PHD\\\\GAN\\\\code.12.2020\\\\PGAN1\\\\hasc\\\\results\\\\test_loss_v2\"+dataset+\".csv\")\n","    \n","    epoch+=1"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["X source shape: (5408, 200, 3)\n","Y source shape: (5408,)\n","P source shape: (5408,)\n","378\n","162\n","(162, 6) (378, 6) (162, 100) (378, 100)\n","Model: \"generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 200, 3)]          0         \n","_________________________________________________________________\n","dropout_33 (Dropout)         (None, 200, 3)            0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 200, 128)          67584     \n","_________________________________________________________________\n","dropout_34 (Dropout)         (None, 200, 128)          0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 200, 3)            387       \n","=================================================================\n","Total params: 67,971\n","Trainable params: 67,971\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"D_a\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_21 (Conv1D)           (None, 198, 64)           640       \n","_________________________________________________________________\n","leaky_re_lu_27 (LeakyReLU)   (None, 198, 64)           0         \n","_________________________________________________________________\n","max_pooling1d_21 (MaxPooling (None, 99, 64)            0         \n","_________________________________________________________________\n","dropout_35 (Dropout)         (None, 99, 64)            0         \n","_________________________________________________________________\n","batch_normalization_24 (Batc (None, 99, 64)            256       \n","_________________________________________________________________\n","conv1d_22 (Conv1D)           (None, 97, 64)            12352     \n","_________________________________________________________________\n","leaky_re_lu_28 (LeakyReLU)   (None, 97, 64)            0         \n","_________________________________________________________________\n","max_pooling1d_22 (MaxPooling (None, 48, 64)            0         \n","_________________________________________________________________\n","dropout_36 (Dropout)         (None, 48, 64)            0         \n","_________________________________________________________________\n","batch_normalization_25 (Batc (None, 48, 64)            256       \n","_________________________________________________________________\n","conv1d_23 (Conv1D)           (None, 46, 64)            12352     \n","_________________________________________________________________\n","leaky_re_lu_29 (LeakyReLU)   (None, 46, 64)            0         \n","_________________________________________________________________\n","max_pooling1d_23 (MaxPooling (None, 23, 64)            0         \n","_________________________________________________________________\n","dropout_37 (Dropout)         (None, 23, 64)            0         \n","_________________________________________________________________\n","conv1d_24 (Conv1D)           (None, 21, 64)            12352     \n","_________________________________________________________________\n","leaky_re_lu_30 (LeakyReLU)   (None, 21, 64)            0         \n","_________________________________________________________________\n","max_pooling1d_24 (MaxPooling (None, 10, 64)            0         \n","_________________________________________________________________\n","dropout_38 (Dropout)         (None, 10, 64)            0         \n","_________________________________________________________________\n","flatten_6 (Flatten)          (None, 640)               0         \n","_________________________________________________________________\n","batch_normalization_26 (Batc (None, 640)               2560      \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 100)               64100     \n","_________________________________________________________________\n","leaky_re_lu_31 (LeakyReLU)   (None, 100)               0         \n","_________________________________________________________________\n","dropout_39 (Dropout)         (None, 100)               0         \n","_________________________________________________________________\n","batch_normalization_27 (Batc (None, 100)               400       \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 6)                 606       \n","=================================================================\n","Total params: 105,874\n","Trainable params: 104,138\n","Non-trainable params: 1,736\n","_________________________________________________________________\n","Model: \"D_p\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv1d_25 (Conv1D)           (None, 198, 64)           640       \n","_________________________________________________________________\n","leaky_re_lu_32 (LeakyReLU)   (None, 198, 64)           0         \n","_________________________________________________________________\n","max_pooling1d_25 (MaxPooling (None, 99, 64)            0         \n","_________________________________________________________________\n","dropout_40 (Dropout)         (None, 99, 64)            0         \n","_________________________________________________________________\n","batch_normalization_28 (Batc (None, 99, 64)            256       \n","_________________________________________________________________\n","conv1d_26 (Conv1D)           (None, 97, 64)            12352     \n","_________________________________________________________________\n","leaky_re_lu_33 (LeakyReLU)   (None, 97, 64)            0         \n","_________________________________________________________________\n","max_pooling1d_26 (MaxPooling (None, 48, 64)            0         \n","_________________________________________________________________\n","dropout_41 (Dropout)         (None, 48, 64)            0         \n","_________________________________________________________________\n","batch_normalization_29 (Batc (None, 48, 64)            256       \n","_________________________________________________________________\n","conv1d_27 (Conv1D)           (None, 46, 64)            12352     \n","_________________________________________________________________\n","leaky_re_lu_34 (LeakyReLU)   (None, 46, 64)            0         \n","_________________________________________________________________\n","max_pooling1d_27 (MaxPooling (None, 23, 64)            0         \n","_________________________________________________________________\n","dropout_42 (Dropout)         (None, 23, 64)            0         \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 1472)              0         \n","_________________________________________________________________\n","batch_normalization_30 (Batc (None, 1472)              5888      \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 100)               147300    \n","_________________________________________________________________\n","leaky_re_lu_35 (LeakyReLU)   (None, 100)               0         \n","_________________________________________________________________\n","dropout_43 (Dropout)         (None, 100)               0         \n","_________________________________________________________________\n","batch_normalization_31 (Batc (None, 100)               400       \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 100)               10100     \n","=================================================================\n","Total params: 189,544\n","Trainable params: 186,144\n","Non-trainable params: 3,400\n","_________________________________________________________________\n","Model: \"model_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 200, 3)]          0         \n","_________________________________________________________________\n","dropout_33 (Dropout)         (None, 200, 3)            0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 200, 128)          67584     \n","_________________________________________________________________\n","dropout_34 (Dropout)         (None, 200, 128)          0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 200, 3)            387       \n","=================================================================\n","Total params: 67,971\n","Trainable params: 67,971\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"model_9\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 200, 3)]     0                                            \n","__________________________________________________________________________________________________\n","dropout_33 (Dropout)            (None, 200, 3)       0           input_4[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   (None, 200, 128)     67584       dropout_33[0][0]                 \n","__________________________________________________________________________________________________\n","dropout_34 (Dropout)            (None, 200, 128)     0           lstm_3[0][0]                     \n","__________________________________________________________________________________________________\n","dense_15 (Dense)                (None, 200, 3)       387         dropout_34[0][0]                 \n","__________________________________________________________________________________________________\n","D_a (Sequential)                (None, 6)            105874      dense_15[0][0]                   \n","__________________________________________________________________________________________________\n","D_p (Sequential)                (None, 100)          189544      dense_15[0][0]                   \n","==================================================================================================\n","Total params: 363,389\n","Trainable params: 67,971\n","Non-trainable params: 295,418\n","__________________________________________________________________________________________________\n","WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x00000031C35314C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_train_function.<locals>.train_function at 0x00000031C35314C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x000000320EF5FE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x000000320EF5FE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_train_function.<locals>.train_function at 0x000000320F066820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_train_function.<locals>.train_function at 0x000000320F066820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","1 train 0.20370370149612427\n","1 test 0.17901234328746796\n"]},{"output_type":"error","ename":"ValueError","evalue":"in user code:\n\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:755 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:186 __call__\n        self.build(y_pred)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:138 build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:62 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:585 map_to_output_names\n        raise ValueError('Found unexpected keys that do not correspond '\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['D_a']). Expected: ['dense_17']\n","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-a99a6c41333b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1000000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[0mdiscriminator_a_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m     discriminator_a_model.compile(loss={\"D_a\":\"categorical_crossentropy\"},\n\u001b[0;32m    285\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"D_a\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1725\u001b[0m                                                     class_weight)\n\u001b[0;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:755 train_step\n        loss = self.compiled_loss(\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:186 __call__\n        self.build(y_pred)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:138 build\n        self._losses = self._conform_to_outputs(y_pred, self._losses)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:62 _conform_to_outputs\n        struct = map_to_output_names(outputs, self._output_names, struct)\n    C:\\Users\\Novotech\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:585 map_to_output_names\n        raise ValueError('Found unexpected keys that do not correspond '\n\n    ValueError: Found unexpected keys that do not correspond to any Model output: dict_keys(['D_a']). Expected: ['dense_17']\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}